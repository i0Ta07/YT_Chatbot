{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1836f726",
   "metadata": {},
   "source": [
    "Chatbot to talk to youtube video in realtime. You can ask literally anything about the video. We will use RAG, will send transcript of the related section as the context along with the query inside the promnpt. Many usecases such as it can summarise them, can solve user's doubts in some part of video. You can ask if the video talks about some topic if yes it can give what they talk about etc. Can implement the UI in yt plugin and also in streamlit.\n",
    "We have to add RestAPI to send and get results. Also apply checks such as 3 min per query etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182705e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2db242",
   "metadata": {},
   "source": [
    "Step1a: Indexing(Document Ingestion)-Hit the Youtube API and get the transcipt in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = \"Gfr50f6ZBvo\"\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "\n",
    "try:\n",
    "    transcript_list = ytt_api.fetch(video_id, languages=[\"en\"])\n",
    "    transript = \"\"\n",
    "    for snippet in transcript_list:\n",
    "        transript +=  \" \" + snippet.text\n",
    "\n",
    "except TranscriptsDisabled:\n",
    "    print(\"No captions available for thi video\")\n",
    "print(transript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162a885",
   "metadata": {},
   "source": [
    "Step-1b: Text splitting \n",
    "You can also create the chunks based on tokens, when you are dealing with larger chunk_size that are closer to embedding model chunk limit. Here we used the recursiveCharacterSplitter and then checks if the output chunks have consistent semantic density. All chunks tokens lie from 173 to 221 except the last. We can clearly see less variability across the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"gpt-5-nano\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "recursuive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "chunks = recursuive_splitter.create_documents([transript]) # Return a list of document objects from the text\n",
    "\n",
    "# Check for consistent token split\n",
    "token_size = {}\n",
    "for i,chunk in enumerate(chunks):\n",
    "    token_size[f\"Chunk{i+1}\"] = count_tokens(chunk.page_content) \n",
    "\n",
    "print(sorted(token_size.values()))\n",
    "print(token_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[0])\n",
    "print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc0339",
   "metadata": {},
   "source": [
    "Step 1c: Create vectors from chunks and store it in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f775ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594eec67",
   "metadata": {},
   "source": [
    "Here 0 is the faiss index in the vector store and the UUID is document id store in the docstore which maps the actual document objects to vectors stored in the FAISS vector_store.\n",
    "{\n",
    "    0: '5e89700d-7a7a-4b83-a307-f7dcc39f3e46',\n",
    "    1: '38f70944-48a2-4b95-a711-bc79d7612a70',\n",
    "    2: '458c111f-4ef8-486c-9fdf-bc412e22f351',\n",
    "    3: 'd0cbdde1-027b-44fb-8143-647e9daa23e3',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00628a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.get_by_ids(['b42ddd41-fb8a-414a-a77d-16f01e7a3cb5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3f475",
   "metadata": {},
   "source": [
    "Step 2: Create retriever and fetch related documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f27b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_retriever = vector_store.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={\"k\":3} # Fetch top 3 most relevant documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input user query, output documents similar/close/relevant to the query\n",
    "similarity_retriever.invoke(\"What is future of Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51cef3",
   "metadata": {},
   "source": [
    "Step 3: Augumentation: Send addtional context for the query to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c38848",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are helpful assistant\n",
    "    ANSWER ONLY FROM THE PROVIDED TRANSCRIPT CONTEXT\n",
    "    If the context is insufficient, just say you don't know\n",
    "    {context}\n",
    "    Question:{question}\n",
    "\"\"\",\n",
    "input_variables=[\"context\",\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is the speaker in the video and how is he related to deepmind\"\n",
    "retrieved_docs = similarity_retriever.invoke(question)\n",
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing context from page_content\n",
    "context = \"\"\n",
    "for doc in retrieved_docs:\n",
    "    context += \"\\n\\n\" + (doc.page_content)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03591c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = template.invoke({\"context\":context,\"question\":question})\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568190d",
   "metadata": {},
   "source": [
    "Step 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "response  = chat_model.invoke(final_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bb4e2",
   "metadata": {},
   "source": [
    "# Creating a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b21c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract page_content from the document objects and create a string\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    context = \"\"\n",
    "    for doc in retrieved_docs:\n",
    "        context += \"\\n\\n\" + (doc.page_content)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eba448",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Who is the speaker in the video and what are his qualifications\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd201c1",
   "metadata": {},
   "source": [
    "We have to perform some pre-processing using format_docs() fucntion. This function can only be a part of the chain if it is a runnable. Then we convert it into a runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the dictionary with {'context': output of first chain,'question', No processing, simply return the input as output}\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"context\": similarity_retriever | RunnableLambda(format_docs),\n",
    "    \"question\": RunnablePassthrough()\n",
    "})\n",
    "parallel_chain.invoke(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df83b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_parser = StrOutputParser()\n",
    "final_chain = parallel_chain | template | chat_model | str_parser\n",
    "\n",
    "result = final_chain.invoke(user_query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
